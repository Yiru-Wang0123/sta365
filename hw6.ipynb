{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0408198-75fe-4ed3-b175-511321ec498b",
   "metadata": {},
   "source": [
    "# STA365 Homework 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c896c6-841b-4eed-a3f4-761645add33a",
   "metadata": {},
   "source": [
    "## Q1: Let's start having some *real* fun...\n",
    "\n",
    "We previously considered the normal-gamma specification \n",
    "\n",
    "$$\\scriptsize\n",
    "\\begin{align*}\n",
    "p(\\theta,\\tau|x) &\\propto{} p(\\theta,\\tau,x) = p(x|\\theta)p(\\theta)p(\\tau) \\quad (\\theta \\perp\\!\\!\\perp \\tau) \\leftarrow \\text{independent priors} & p(\\theta|x,\\theta_0,\\tau_0, \\tau) &={} \\text{N}\\left(\\frac{\\left(\\tau_0 \\theta_0+\\tau\\sum_{i=1}^{n}x_{i}\\right)}{(\\tau_0+n\\tau)}, \\sigma^{-2}=\\tau_0+n\\tau \\right)\\\\\n",
    "&={}  \\left[\\prod_{i=1}^n\\sqrt{\\frac{\\tau}{2\\pi}} e^{-\\frac{\\tau\\left(x_i-\\theta\\right)^2}{2}}\\right] \\sqrt{\\frac{\\tau_0}{2\\pi}} e^{-\\frac{\\tau_0\\left(\\theta-\\theta_0\\right)^2}{2}} \\frac{\\beta ^{\\alpha}}{\\Gamma(\\alpha)} \\tau^{\\alpha -1}e^{-\\beta \\tau} & p(\\tau|x, \\alpha, \\beta, \\theta) &={} \\text{Gamma}\\left(\\frac{\\alpha}{2}+\\frac{n}{2}, \\frac{\\beta}{2}+\\frac{1}{2}\\sum_{i=1}^n\\left(x_i-\\theta\\right)^2 \\right)\\\\{}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "How about instead we consider a \"[location-scale-t](https://en.wikipedia.org/wiki/Student%27s_t-distribution#Location-scale_t-distribution)-norm-halfnorm-discrete-uniform\" specification?\n",
    "\n",
    "$$\\large\n",
    "\\overset{x_i\\; \\sim\\; \\text{location-scale-t}(\\mu, \\sigma^2, \\nu)}{\\quad\\quad\\quad p(x|\\mu,\\sigma^2, \\nu)} = {\\prod_{i=1}^n\n",
    "\\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right) \\sqrt{\\pi \\nu \\sigma^2}}\\left(1+\\frac{1}{\\nu} \\frac{(x_i-\\mu)^2}{\\sigma^2}\\right)^{-(\\nu+1) / 2}}$$\n",
    "\n",
    "$$\\scriptsize \n",
    "\\begin{align}\n",
    "p(\\mu | \\mu_0, \\tau_0) &={} \\sqrt{\\frac{\\tau_0}{2\\pi}} e^{-\\frac{\\tau_0}{2}\\left(\\mu-\\mu_0\\right)^2} & p(\\sigma^2 | \\sigma_0^2) &={} \\sqrt{\\frac{2}{\\pi\\sigma_0^2}} \\exp \\left(-\\frac{(\\sigma^2)^2}{2 \\sigma_0^2}\\right) 1_{[0,\\infty]}(\\sigma^2) & p(\\nu=i) &={} \\Bigg\\{ \\begin{array}{cl} \\frac{1}{100} & \\text{for }i=1,\\cdots,100\\\\ 0 & \\text{otherwise} \\end{array}\\\\\n",
    "& \\textrm{normal} && \\textrm{half-normal} && \\textrm{discrete uniform}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Um yeah we're gonna need a Metroposlis cleanup on aisles one two and three  \n",
    "(or a slice or adapative squeeze rejection sampling steps... in place of Metroposlis steps)\n",
    "\n",
    "*Implement the a Metroposlis within Gibbs algorithm to smaple from the posterior of the above specification. Use a \"smallish\" sample size, say $n=100$ and implement your acceptance probability on a log-scale as described in [piazza post @65_f1](https://piazza.com/class/m5jvyco84083fm/post/65_f1)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfb5b6-22a9-4faf-bdf2-1e7c12a92808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Generate synthetic data from a t-distribution\n",
    "np.random.seed(123)\n",
    "n = 100  # Sample size\n",
    "true_mu = 0.5\n",
    "true_sigma2 = 2.0\n",
    "true_nu = 5\n",
    "\n",
    "# Generate data from a t-distribution\n",
    "x = stats.t(df=true_nu, loc=true_mu, scale=np.sqrt(true_sigma2)).rvs(n)\n",
    "\n",
    "# Prior hyperparameters\n",
    "mu0 = 0.0\n",
    "tau0 = 1.0\n",
    "sigma0_2 = 5.0\n",
    "\n",
    "# Metropolis-within-Gibbs settings\n",
    "n_iterations = 5000\n",
    "burnin = 1000\n",
    "\n",
    "# Initialize arrays to store samples\n",
    "mu_samples = np.zeros(n_iterations)\n",
    "sigma2_samples = np.zeros(n_iterations)\n",
    "nu_samples = np.zeros(n_iterations, dtype=int)\n",
    "\n",
    "# Initialize parameters\n",
    "mu_samples[0] = 0.0\n",
    "sigma2_samples[0] = 1.0\n",
    "nu_samples[0] = 10\n",
    "\n",
    "# Functions for log-posteriors\n",
    "def log_likelihood(x, mu, sigma2, nu):\n",
    "    \"\"\"Log likelihood for t-distribution\"\"\"\n",
    "    n = len(x)\n",
    "    log_lik = (\n",
    "        n * (\n",
    "            gammaln((nu + 1) / 2) - \n",
    "            gammaln(nu / 2) -\n",
    "            0.5 * np.log(np.pi * nu * sigma2)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    for i in range(n):\n",
    "        log_lik -= ((nu + 1) / 2) * np.log(1 + (1 / nu) * ((x[i] - mu) ** 2) / sigma2)\n",
    "    \n",
    "    return log_lik\n",
    "\n",
    "def log_prior_mu(mu, mu0, tau0):\n",
    "    \"\"\"Log prior for μ - Normal\"\"\"\n",
    "    return stats.norm(mu0, 1 / np.sqrt(tau0)).logpdf(mu)\n",
    "\n",
    "def log_prior_sigma2(sigma2, sigma0_2):\n",
    "    \"\"\"Log prior for σ² - Half-Normal\"\"\"\n",
    "    if sigma2 <= 0:\n",
    "        return -np.inf\n",
    "    return np.log(np.sqrt(2 / (np.pi * sigma0_2))) - (sigma2 ** 2) / (2 * sigma0_2)\n",
    "\n",
    "def log_prior_nu(nu):\n",
    "    \"\"\"Log prior for ν - Discrete Uniform\"\"\"\n",
    "    if 1 <= nu <= 100:\n",
    "        return np.log(1/100)\n",
    "    return -np.inf\n",
    "\n",
    "def log_posterior_mu(mu, sigma2, nu, x, mu0, tau0):\n",
    "    \"\"\"Log posterior for μ\"\"\"\n",
    "    return log_likelihood(x, mu, sigma2, nu) + log_prior_mu(mu, mu0, tau0)\n",
    "\n",
    "def log_posterior_sigma2(mu, sigma2, nu, x, sigma0_2):\n",
    "    \"\"\"Log posterior for σ²\"\"\"\n",
    "    return log_likelihood(x, mu, sigma2, nu) + log_prior_sigma2(sigma2, sigma0_2)\n",
    "\n",
    "def log_posterior_nu(mu, sigma2, nu, x):\n",
    "    \"\"\"Log posterior for ν\"\"\"\n",
    "    return log_likelihood(x, mu, sigma2, nu) + log_prior_nu(nu)\n",
    "\n",
    "# Metropolis-within-Gibbs algorithm\n",
    "for t in tqdm(range(1, n_iterations)):\n",
    "    # Current values\n",
    "    mu_current = mu_samples[t-1]\n",
    "    sigma2_current = sigma2_samples[t-1]\n",
    "    nu_current = nu_samples[t-1]\n",
    "    \n",
    "    # Step 1: Update μ using Metropolis\n",
    "    mu_proposal = stats.norm(mu_current, 0.2).rvs()\n",
    "    log_accept_ratio = log_posterior_mu(mu_proposal, sigma2_current, nu_current, x, mu0, tau0) - \\\n",
    "                       log_posterior_mu(mu_current, sigma2_current, nu_current, x, mu0, tau0)\n",
    "    \n",
    "    if np.log(np.random.uniform()) < log_accept_ratio:\n",
    "        mu_samples[t] = mu_proposal\n",
    "    else:\n",
    "        mu_samples[t] = mu_current\n",
    "    \n",
    "    # Step 2: Update σ² using Metropolis\n",
    "    # Using log-normal proposal to ensure positivity\n",
    "    sigma2_proposal = sigma2_current * np.exp(stats.norm(0, 0.2).rvs())\n",
    "    log_accept_ratio = log_posterior_sigma2(mu_samples[t], sigma2_proposal, nu_current, x, sigma0_2) - \\\n",
    "                       log_posterior_sigma2(mu_samples[t], sigma2_current, nu_current, x, sigma0_2)\n",
    "    \n",
    "    # Adjustment for asymmetric proposal\n",
    "    log_accept_ratio += np.log(sigma2_proposal) - np.log(sigma2_current)\n",
    "    \n",
    "    if np.log(np.random.uniform()) < log_accept_ratio:\n",
    "        sigma2_samples[t] = sigma2_proposal\n",
    "    else:\n",
    "        sigma2_samples[t] = sigma2_current\n",
    "    \n",
    "    # Step 3: Update ν using Metropolis on integers\n",
    "    # Propose a move in {-1, 0, 1} to maintain locality\n",
    "    nu_step = np.random.choice([-1, 0, 1])\n",
    "    nu_proposal = max(1, min(100, nu_current + nu_step))\n",
    "    \n",
    "    if nu_proposal != nu_current:\n",
    "        log_accept_ratio = log_posterior_nu(mu_samples[t], sigma2_samples[t], nu_proposal, x) - \\\n",
    "                          log_posterior_nu(mu_samples[t], sigma2_samples[t], nu_current, x)\n",
    "        \n",
    "        if np.log(np.random.uniform()) < log_accept_ratio:\n",
    "            nu_samples[t] = nu_proposal\n",
    "        else:\n",
    "            nu_samples[t] = nu_current\n",
    "    else:\n",
    "        nu_samples[t] = nu_current\n",
    "\n",
    "# Discard burn-in\n",
    "mu_samples = mu_samples[burnin:]\n",
    "sigma2_samples = sigma2_samples[burnin:]\n",
    "nu_samples = nu_samples[burnin:]\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "\n",
    "# Trace plots\n",
    "axes[0, 0].plot(mu_samples)\n",
    "axes[0, 0].set_title('Trace of μ')\n",
    "axes[0, 0].axhline(true_mu, color='r', linestyle='--')\n",
    "\n",
    "axes[1, 0].plot(sigma2_samples)\n",
    "axes[1, 0].set_title('Trace of σ²')\n",
    "axes[1, 0].axhline(true_sigma2, color='r', linestyle='--')\n",
    "\n",
    "axes[2, 0].plot(nu_samples)\n",
    "axes[2, 0].set_title('Trace of ν')\n",
    "axes[2, 0].axhline(true_nu, color='r', linestyle='--')\n",
    "\n",
    "# Histograms\n",
    "axes[0, 1].hist(mu_samples, bins=30, density=True)\n",
    "axes[0, 1].set_title('Posterior of μ')\n",
    "axes[0, 1].axvline(true_mu, color='r', linestyle='--')\n",
    "\n",
    "axes[1, 1].hist(sigma2_samples, bins=30, density=True)\n",
    "axes[1, 1].set_title('Posterior of σ²')\n",
    "axes[1, 1].axvline(true_sigma2, color='r', linestyle='--')\n",
    "\n",
    "axes[2, 1].hist(nu_samples, bins=np.arange(1, 21), density=True)\n",
    "axes[2, 1].set_title('Posterior of ν')\n",
    "axes[2, 1].axvline(true_nu, color='r', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate posterior means and credible intervals\n",
    "print(f\"True μ: {true_mu:.4f}, Posterior mean: {np.mean(mu_samples):.4f}, 95% CI: [{np.percentile(mu_samples, 2.5):.4f}, {np.percentile(mu_samples, 97.5):.4f}]\")\n",
    "print(f\"True σ²: {true_sigma2:.4f}, Posterior mean: {np.mean(sigma2_samples):.4f}, 95% CI: [{np.percentile(sigma2_samples, 2.5):.4f}, {np.percentile(sigma2_samples, 97.5):.4f}]\")\n",
    "print(f\"True ν: {true_nu}, Posterior mode: {stats.mode(nu_samples).mode}, 95% CI: [{np.percentile(nu_samples, 2.5):.1f}, {np.percentile(nu_samples, 97.5):.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac03b0-bf9f-4061-891c-dd71bb2e76df",
   "metadata": {},
   "source": [
    "### Q2: explore the role of sample size in providing inference for the degrees of freedom parameter $\\nu$\n",
    "\n",
    "*Implement the specification above using `PyMC` where you can explore inference on $\\nu$ at different sample sizes. Provide a summarization and explanation of your findings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93fb21-eb05-4d6a-91bf-401850c8aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# True parameter values\n",
    "true_mu = 0.5\n",
    "true_sigma2 = 2.0\n",
    "true_nu = 5\n",
    "\n",
    "# Function to generate data and run inference\n",
    "def run_inference_with_sample_size(n_samples, n_draws=2000):\n",
    "    \"\"\"\n",
    "    Generate t-distributed data with given sample size and run Bayesian inference\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of data points to generate\n",
    "    n_draws : int\n",
    "        Number of posterior samples to draw\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    InferenceData object with posterior samples\n",
    "    \"\"\"\n",
    "    # Generate data\n",
    "    x = stats.t(df=true_nu, loc=true_mu, scale=np.sqrt(true_sigma2)).rvs(n_samples)\n",
    "    \n",
    "    # Create PyMC model\n",
    "    with pm.Model() as model:\n",
    "        # Priors\n",
    "        mu = pm.Normal('mu', mu=0.0, sigma=1.0)\n",
    "        sigma2 = pm.HalfNormal('sigma2', sigma=np.sqrt(5.0))\n",
    "        \n",
    "        # For ν, we'll use a DiscreteUniform prior from 1 to 100\n",
    "        nu = pm.DiscreteUniform('nu', lower=1, upper=100)\n",
    "        \n",
    "        # Likelihood\n",
    "        likelihood = pm.StudentT('likelihood', nu=nu, mu=mu, sigma=np.sqrt(sigma2), observed=x)\n",
    "        \n",
    "        # Sample from the posterior\n",
    "        idata = pm.sample(n_draws, tune=1000, return_inferencedata=True,\n",
    "                          target_accept=0.9, chains=2)\n",
    "        \n",
    "    return idata\n",
    "\n",
    "# Sample sizes to test\n",
    "sample_sizes = [20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Run inference for each sample size\n",
    "for n in sample_sizes:\n",
    "    print(f\"Running inference with sample size = {n}\")\n",
    "    results[n] = run_inference_with_sample_size(n)\n",
    "\n",
    "# Analyze the results\n",
    "nu_summary = pd.DataFrame(index=sample_sizes, \n",
    "                         columns=['Mean', 'Median', 'Mode', 'SD', 'HDI_2.5%', 'HDI_97.5%'])\n",
    "\n",
    "for n in sample_sizes:\n",
    "    posterior_nu = results[n].posterior.nu.values.flatten()\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    nu_summary.loc[n, 'Mean'] = np.mean(posterior_nu)\n",
    "    nu_summary.loc[n, 'Median'] = np.median(posterior_nu)\n",
    "    \n",
    "    # Fix for stats.mode() in newer scipy versions\n",
    "    mode_result = stats.mode(posterior_nu)\n",
    "    if hasattr(mode_result, 'mode'):\n",
    "        # New scipy version returns an object with 'mode' attribute\n",
    "        if isinstance(mode_result.mode, np.ndarray):\n",
    "            nu_summary.loc[n, 'Mode'] = mode_result.mode[0]\n",
    "        else:\n",
    "            nu_summary.loc[n, 'Mode'] = mode_result.mode\n",
    "    else:\n",
    "        # Older scipy returns a tuple\n",
    "        nu_summary.loc[n, 'Mode'] = mode_result[0][0]\n",
    "    \n",
    "    nu_summary.loc[n, 'SD'] = np.std(posterior_nu)\n",
    "    nu_summary.loc[n, 'HDI_2.5%'] = np.percentile(posterior_nu, 2.5)\n",
    "    nu_summary.loc[n, 'HDI_97.5%'] = np.percentile(posterior_nu, 97.5)\n",
    "    \n",
    "# Create visualization of results\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Posterior distributions by sample size\n",
    "plt.subplot(2, 2, 1)\n",
    "for n in sample_sizes:\n",
    "    sns.kdeplot(results[n].posterior.nu.values.flatten(), label=f'n={n}')\n",
    "plt.axvline(true_nu, color='red', linestyle='--', label='True ν')\n",
    "plt.title('Posterior Distribution of ν by Sample Size')\n",
    "plt.xlabel('ν')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Credible interval width vs sample size\n",
    "plt.subplot(2, 2, 2)\n",
    "interval_width = nu_summary['HDI_97.5%'] - nu_summary['HDI_2.5%']\n",
    "plt.plot(sample_sizes, interval_width, 'o-')\n",
    "plt.title('95% Credible Interval Width vs Sample Size')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Width of 95% Credible Interval')\n",
    "plt.xscale('log')\n",
    "\n",
    "# Plot 3: Posterior standard deviation vs sample size\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(sample_sizes, nu_summary['SD'], 'o-')\n",
    "plt.title('Posterior Standard Deviation vs Sample Size')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.xscale('log')\n",
    "\n",
    "# Plot 4: Box plots of posterior distributions\n",
    "plt.subplot(2, 2, 4)\n",
    "box_data = [results[n].posterior.nu.values.flatten() for n in sample_sizes]\n",
    "plt.boxplot(box_data, labels=sample_sizes)\n",
    "plt.axhline(true_nu, color='red', linestyle='--')\n",
    "plt.title('Boxplot of Posterior Distributions by Sample Size')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('ν')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"Summary of Posterior Distributions for ν:\")\n",
    "print(nu_summary)\n",
    "\n",
    "# Calculate absolute error from true value\n",
    "nu_summary['Absolute Error'] = abs(nu_summary['Mean'] - true_nu)\n",
    "print(\"\\nAbsolute Error from True Value:\")\n",
    "print(nu_summary['Absolute Error'])\n",
    "\n",
    "# Calculate relative reduction in uncertainty\n",
    "print(\"\\nRelative Reduction in Uncertainty (compared to n=20):\")\n",
    "base_width = nu_summary.loc[20, 'HDI_97.5%'] - nu_summary.loc[20, 'HDI_2.5%']\n",
    "for n in sample_sizes[1:]:\n",
    "    current_width = nu_summary.loc[n, 'HDI_97.5%'] - nu_summary.loc[n, 'HDI_2.5%']\n",
    "    reduction = (base_width - current_width) / base_width * 100\n",
    "    print(f\"n={n}: {reduction:.2f}% reduction in 95% credible interval width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0e498-118f-4513-b7e9-ba12ff5eb2a4",
   "metadata": {},
   "source": [
    "### Q3: the posterior predictive distribution does something like the following...\n",
    "\n",
    "Let $p(\\tau_i)$ be $\\require{cancel} \\textrm{gamma}\\big(\\tau_i | \\alpha = \\frac{\\nu}{2}, \\overset{\\textrm{rate}\\xcancel{\\textrm{scale}}}{\\beta = \\frac{\\nu}{2}}\\big)$ and let $p(y_i|\\tau_i)$ be $\\textrm{N}(y_i | 0,\\tau_i)$ and now integrate out the uncertainty in $\\tau_i$ and see what distribution is left over for $y_i$.\n",
    "\n",
    "*Go look at the gamma distribution and remember that you know that the integrals of unnormalized densities are the inverse of their normalizing constants. Then go look at the t distribution and determine what distribution the following expression defines. Then explain why the behavior demonstrated here is analagous to that of the posterior predictive distribution.*\n",
    "\n",
    "$$\\int p(y_i|\\tau_i) p(\\tau_i)  d\\tau_i = \\int \\sqrt{\\frac{\\tau_i}{2\\pi}}e^{-\\frac{1}{2}\\tau_i y_i^2} \\frac {\\frac{\\nu}{2}^{\\frac{\\nu}{2}}}{\\Gamma \\left(\\frac{\\nu}{2}\\right)} \\tau_i^{\\frac{\\nu}{2}-1}e^{-\\frac{\\nu}{2}\\tau_i} d\\tau_i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456bafc-bd8d-4154-bf84-dd9880bdf446",
   "metadata": {},
   "source": [
    "![title](img/hw6_q3.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
